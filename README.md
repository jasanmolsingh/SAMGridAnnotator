[![DOI](https://zenodo.org/badge/1154590897.svg)](https://doi.org/10.5281/zenodo.18600184)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)


# ğŸŒ¿ SAMGridAnnotations

**Interactive Image Annotation using Meta's Segment Anything Model (SAM)**

---

## ğŸ§  What is this?

This repository helps you create **segmentation masks** for images using Meta AIâ€™s powerful **Segment Anything Model (SAM)**.

ğŸ‘‰ In simple words:

Instead of manually drawing shapes around plants, soil, weeds, etcâ€¦

1ï¸âƒ£ The program scans the image.

<img width="1920" height="1080" alt="0GridCreation_GridPointActivation" src="https://github.com/user-attachments/assets/e1e71121-0f63-4663-a6ca-77a3680ecc49" />

2ï¸âƒ£ The AI suggests masks

<img width="1920" height="1080" alt="1MaskSelection" src="https://github.com/user-attachments/assets/2e81e166-db7a-4d31-bbf3-ad63fe797895" />

3ï¸âƒ£ You click the best one and choose a class

<img width="1920" height="1080" alt="2ClassSelection" src="https://github.com/user-attachments/assets/0006a00e-38e2-4a76-a333-dba83f0b9024" />

4ï¸âƒ£ Selected mask is saved. Continue!

<img width="1920" height="1080" alt="3Selectedmask_Saved" src="https://github.com/user-attachments/assets/bc42e3b8-27c4-4bc0-8349-08f6114b6ffb" />


5ï¸âƒ£ Done! Final mask of the image saves after all gridpoints are complete. 


This makes dataset creation MUCH faster.

---
## â˜˜ï¸ Current Usage

âœ³ï¸ This code is generated specifically for forage mixtures with **Alfalfa, Tall fescue, White clover, Weed, Backgound and Soil** as classes. For other objects, please change the classes and labels in the code.

---

## ğŸ¯ What does this tool do?

For every image:

1ï¸âƒ£ Places a smart grid of points on the image 

2ï¸âƒ£ AI generates **3 mask options** 

3ï¸âƒ£ You select the best mask 

4ï¸âƒ£ Choose the correct label (Weed, Soil, etc.) 

5ï¸âƒ£ Mask is automatically saved 

ğŸ’¡ You can stop anytime â€” the tool resumes from where you left off!

---

## âš™ï¸ Requirements (VERY IMPORTANT)

Install:

* Python **3.9 or 3.10 recommended**
* GPU is optional but strongly recommended (makes SAM MUCH faster)

---

## ğŸ“¥ Step 1 â€” Clone this Repository

```bash
git clone https://github.com/jasanmolsingh/SAMGridAnnotations.git
cd SAMGridAnnotations
```
---

## ğŸ“¦ Step 2 â€” Install Dependencies

Create a virtual environment (recommended):

### âœ… Windows

```bash
python -m venv SAMGridAnnotations
SAMGridAnnotations\Scripts\activate
```

### âœ… Mac/Linux

```bash
python3 -m venv SAMGridAnnotations
source SAMGridAnnotations/bin/activate
```

Now install packages:

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Step 3 â€” Download SAM Model (REQUIRED)

âš ï¸ The tool **WILL NOT work** without this.

Download the checkpoint:

ğŸ‘‰ [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything)

Download:

```
sam_vit_b.pth
```

âœï¸ Rename the downloaded file to "sam_vit_b.pth", if original filename is different.


Create a folder:

```
weights/
```

Put the file inside:

```
weights/sam_vit_b.pth
```



---

## ğŸ“ Step 4 â€” Folder Structure

Your project should look like this:

```
SAMGridAnnotations/
â”‚
â”œâ”€â”€ SAMGridAnnotations.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ weights/
â”‚     sam_vit_b.pth
â”‚
â””â”€â”€ dataset_mixture/
      â”œâ”€â”€ images/
      â”‚     â””â”€â”€ train/
      â”‚           your_images_here.jpg
      â”‚
      â””â”€â”€ masks/
            â””â”€â”€ train/
                  (auto-created)
```

ğŸ‘‰ Just drop your images inside:

```
dataset_mixture/images/train/
```

---

## â–¶ï¸ Step 5 â€” Run the Notebook

Start Jupyter:

```bash
jupyter notebook
```

Open:

```
SAMGridAnnotations.ipynb
```

Click:

```
Run â†’ Run All Cells
```

---

## ğŸ–±ï¸ How Annotation Works (SUPER SIMPLE)

### âœ… Blue dots appear

These are the points SAM will test.

### âœ… One dot turns RED

This is the active point.

### âœ… AI shows 3 masks

Pick the best one â€” or skip.

### âœ… Choose a class

A popup lets you select:

* Weed
* Soil
* Alfalfa
* White Clover
* Tall Fescue
* Background

DONE âœ…

---

## ğŸ’¾ What Gets Saved?

For every annotation you accept:

### âœ”ï¸ Grayscale Mask

Pixel numbers represent classes.

Example:

```
image1_class2_mask.png
```

---

### âœ”ï¸ Colored Mask with Legend

Helps you visually confirm labels.

```
image1_rgb_legend.png
```

---

## ğŸ” Can I Stop and Resume?

YES ğŸ‘

The tool tracks progress automatically.

If it crashes or you stop:

ğŸ‘‰ Restart it â€” it continues where you left off.

No work is lost.

---

## ğŸš¨ Common Problems

### âŒ SAM not loading?

Check that:

```
weights/sam_vit_b.pth
```

exists.

---

### âŒ Windows popup not showing?

Run locally â€” not on remote servers.

Tkinter needs a GUI.

---

### âŒ CUDA not detected?

Install the GPU version of PyTorch:

ğŸ‘‰ [https://pytorch.org/](https://pytorch.org/)

---

## â­ Pro Tips (Highly Recommended)

âœ” Use a GPU
âœ” Start with 5â€“10 images to test
âœ” Keep backups of masks
âœ” Do NOT rename output files

---

## ğŸ™Œ Credits

* Meta AI â€” Segment Anything Model
* OpenCV
* PyTorch
---

## ğŸ™Œ Acknowledgement

The research work and data collection for this project was conducted in the KAMS Lab (Dept. of Agricultural Sciences, Clemson University). 

KAMS Lab (https://bulent931.wixsite.com/bulent/kams-lab)

---

## ğŸ”¬ Reproducibility

This repository is archived with Zenodo and assigned a DOI to ensure long-term reproducibility of the software.

To reproduce results:

1. Clone the repository
2. Install dependencies from `requirements.txt`
3. Download SAM weights
4. Run the notebook

For version-specific reproduction, use the archived release linked via DOI.

---

## ğŸ‘¤ Author
Jasanmol Singh, M.Sc.

Ph.D. Student | Clemson University, Clemson, SC, USA

[Email](jasanms@clemson.edu) | [Personal Website](https://sites.google.com/view/jasanmolsingh/about)

---

## ğŸ’¬ Questions?

Open a GitHub issue and Iâ€™ll help!
