[![DOI](https://zenodo.org/badge/1154590897.svg)](https://doi.org/10.5281/zenodo.18600184)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)


# ğŸŒ¿ SAMGridAnnotations

**Interactive Image Annotation using Meta's Segment Anything Model (SAM)**

---

## ğŸ§  What is this?

This repository helps you create **segmentation masks** for images using Meta AIâ€™s powerful **Segment Anything Model (SAM)**.

ğŸ‘‰ In simple words:

Instead of manually drawing shapes around plants, soil, weeds, etcâ€¦

âœ… The AI suggests masks
âœ… You click the best one
âœ… Choose a class
âœ… Done!

This makes dataset creation MUCH faster.

---
## â˜˜ï¸ Current Usage

âœ³ï¸ This code is generated specifically for forage mixtures with **Alfalfa, Tall fescue, White clover, Weed, Backgound and Soil** as classes. For other objects, please change the classes and labels in the code.

---

## ğŸ¯ What does this tool do?

For every image:

1ï¸âƒ£ Places a smart grid of points on the image 
2ï¸âƒ£ AI generates **3 mask options** 
3ï¸âƒ£ You select the best mask 
4ï¸âƒ£ Choose the correct label (Weed, Soil, etc.) 
5ï¸âƒ£ Mask is automatically saved 

ğŸ’¡ You can stop anytime â€” the tool resumes from where you left off!

---

## âš™ï¸ Requirements (VERY IMPORTANT)

Install:

* Python **3.9 or 3.10 recommended**
* GPU is optional but strongly recommended (makes SAM MUCH faster)

---

## ğŸ“¥ Step 1 â€” Clone this Repository

```bash
git clone https://github.com/jasanmolsingh/SAMGridAnnotations.git
cd SAMGridAnnotations
```
---

## ğŸ“¦ Step 2 â€” Install Dependencies

Create a virtual environment (recommended):

### âœ… Windows

```bash
python -m venv SAMGridAnnotations
SAMGridAnnotations\Scripts\activate
```

### âœ… Mac/Linux

```bash
python3 -m venv SAMGridAnnotations
source SAMGridAnnotations/bin/activate
```

Now install packages:

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Step 3 â€” Download SAM Model (REQUIRED)

âš ï¸ The tool **WILL NOT work** without this.

Download the checkpoint:

ğŸ‘‰ [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything)

Download:

```
sam_vit_b.pth
```

Create a folder:

```
weights/
```

Put the file inside:

```
weights/sam_vit_b.pth
```
---

## ğŸ“ Step 4 â€” Folder Structure

Your project should look like this:

```
SAMGridAnnotations/
â”‚
â”œâ”€â”€ SAMGridAnnotations.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ weights/
â”‚     sam_vit_b.pth
â”‚
â””â”€â”€ dataset_mixture/
      â”œâ”€â”€ images/
      â”‚     â””â”€â”€ train/
      â”‚           your_images_here.jpg
      â”‚
      â””â”€â”€ masks/
            â””â”€â”€ train/
                  (auto-created)
```

ğŸ‘‰ Just drop your images inside:

```
dataset_mixture/images/train/
```

---

## â–¶ï¸ Step 5 â€” Run the Notebook

Start Jupyter:

```bash
jupyter notebook
```

Open:

```
SAMGridAnnotations.ipynb
```

Click:

```
Run â†’ Run All Cells
```

---

## ğŸ–±ï¸ How Annotation Works (SUPER SIMPLE)

### âœ… Blue dots appear

These are the points SAM will test.

### âœ… One dot turns RED

This is the active point.

### âœ… AI shows 3 masks

Pick the best one â€” or skip.

### âœ… Choose a class

A popup lets you select:

* Weed
* Soil
* Alfalfa
* White Clover
* Tall Fescue
* Background

DONE âœ…

---

## ğŸ’¾ What Gets Saved?

For every annotation you accept:

### âœ”ï¸ Grayscale Mask

Pixel numbers represent classes.

Example:

```
image1_class2_mask.png
```

---

### âœ”ï¸ Colored Mask with Legend

Helps you visually confirm labels.

```
image1_rgb_legend.png
```

---

## ğŸ” Can I Stop and Resume?

YES ğŸ‘

The tool tracks progress automatically.

If it crashes or you stop:

ğŸ‘‰ Restart it â€” it continues where you left off.

No work is lost.

---

## ğŸš¨ Common Problems

### âŒ SAM not loading?

Check that:

```
weights/sam_vit_b.pth
```

exists.

---

### âŒ Windows popup not showing?

Run locally â€” not on remote servers.

Tkinter needs a GUI.

---

### âŒ CUDA not detected?

Install the GPU version of PyTorch:

ğŸ‘‰ [https://pytorch.org/](https://pytorch.org/)

---

## â­ Pro Tips (Highly Recommended)

âœ” Use a GPU
âœ” Start with 5â€“10 images to test
âœ” Keep backups of masks
âœ” Do NOT rename output files

---

## ğŸ™Œ Credits

* Meta AI â€” Segment Anything Model
* OpenCV
* PyTorch
---

## ğŸ™Œ Acknowledgement

* KAMS Lab (https://bulent931.wixsite.com/bulent/kams-lab)

---

## ğŸ”¬ Reproducibility

This repository is archived with Zenodo and assigned a DOI to ensure long-term reproducibility of the software.

To reproduce results:

1. Clone the repository
2. Install dependencies from `requirements.txt`
3. Download SAM weights
4. Run the notebook

For version-specific reproduction, use the archived release linked via DOI.

---

## ğŸ’¬ Questions?

Open a GitHub issue and Iâ€™ll help!
